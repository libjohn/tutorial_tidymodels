---
title: "learning tidymodels"
output: html_notebook
---

from:  https://jhudatascience.org/tidyversecourse/model.html#the-tidymodels-ecosystem-1

```{r}
# library(tidyverse)
library(tidymodels)
```

**Machine Learning Steps**

[5.12.2 -- Prediction modeling in _ts4ds_](https://jhudatascience.org/tidyversecourse/model.html#machine-learning-steps)

1. Data Splitting - what data are you going to use to train your model? To tune your model? To test your model?
1. Variable Selection - what variable(s) from the data you have now are you going to use to predict future outcomes?
1. Model Selection - How are you going to model the data?
1. Accuracy Assessment - How are you going to assess accuracy of your predictions?

[Examples of continuous variable prediction in _ts4ds_](https://jhudatascience.org/tidyversecourse/model.html#example-of-continuous-variable-prediction)

## Step 1. Splitting data

5.13.3.1 - `rsample``

```{r}
# library(rsample)
set.seed(1234)
split_iris <-initial_split(iris, prop = 2/3) 
split_iris
```

training/testing/full

> the default proportion is 1/4 testing and 3/4 training


We can then extract the training and testing datasets by using the `training()` and `testing()` functions, also of the rsample package.

```{r}

training_iris <-training(split_iris)
head(training_iris)
```

```{r}
testing_iris <-testing(split_iris)
head(testing_iris)
```


## Step 2. recipes/steps

aka **featre engineering**

5.12.3.2.1 - `recipes`
5.12.3.2.2 - `steps*`

### `recipe`

using formula notation:  _outcome(s) ~ predictor(s)_

... try to predict `Sepal.Length` in our training data based on `Sepal.Width` and the `Species`. Thus, `Sepal.Length` is our **outcome variable** and `Sepal.Width` and `Species` are our **predictor variables**.


```{r}
first_recipe <- training_iris %>%
                  recipe(Sepal.Length ~ Sepal.Width + Species)

# first_recipe <- recipe(training_iris) %>%
#                   recipes::update_role(Sepal.Length, new_role = "outcome")  %>%
#                   recipes::update_role(Sepal.Width, new_role = "predictor") %>%
#                   recipes::update_role(Species, new_role = "predictor")

first_recipe
```


View recipe

```{r}
summary(first_recipe)
```
### `steps*`


 one-hot encode some of our categorical variables so that they can be used with certain algorithms like a linear regression require numeric predictors.  use the `step_dummy()` function and the `one_hot = TRUE` argument.
 
 
```{r}
first_recipe <- first_recipe %>%
  step_dummy(Species, one_hot = TRUE)

first_recipe
```
 
## Step 3. preprocessing (optional)

### `prep()`
 

```{r}
prepped_rec <- prep(first_recipe, verbose = TRUE, retain = TRUE )
prepped_rec
```

```{r}
names(prepped_rec)
```


```{r}
prepped_rec$var_info
```

Nest step replaces the `Species` variable with 3 variables representing the 3 different species numerically: with zeros and ones.


```{r}
preproc_train <-recipes::bake(prepped_rec, new_data = NULL)
glimpse(preproc_train)
# glimpse(iris)
```

## Step 4. model

`parsnip()` is like an update of `caret()`

specify

1. type (e.g. `rand_forest` / `logisistic_reg`)
1. engine  -  `set_engine` 
3. mode  -  `set_mode`
4. arguments  - specific/necessary for the mode/package selected in 3. 


For our case, we are going to start our analysis with a linear regression but we will demonstrate how we can try different models.

The first step is to define what type of model we would like to use. See [parsip models](https://www.tidymodels.org/find/parsnip/) for modeling options in `parsnip.`


```{r}
Lin_reg_model <- parsnip::linear_reg()
Lin_reg_model
```

So far, all we have defined is that we want to use a linear regression
Now let’s tell parsnip more about what we want.


```{r}
Lin_reg_model <- 
  Lin_reg_model  %>%
  parsnip::set_engine("lm")

Lin_reg_model
```

specify classification or regression.
we aim to predict a continuous variqable, thus we want to perform a regression

```{r}
Lin_reg_model <- 
  Lin_reg_model %>%
  parsnip::set_engine("lm") %>%
  parsnip::set_mode("regression")

Lin_reg_model
```


## Step 5. fit the model

use `parsnip` with `workflows`

begin by creating a workflow using the `workflow()` function

Next, we use `add_recipe()` (our preprocessing specifications) and we add our model with the `add_model()` function – both functions from the workflows package.


**Note**: We do not need to actually `prep()` our recipe before using workflows - this was just optional so we could take a look at the preprocessed data!


```{r}
iris_reg_wflow <-workflows::workflow() %>%
                 workflows::add_recipe(first_recipe) %>%
                 workflows::add_model(Lin_reg_model)
iris_reg_wflow
```


Next, we “prepare the recipe” (or estimate the parameters) and fit the model to our training data all at once. Printing the output, we can see the coefficients of the model.


```{r}
iris_reg_wflow_fit <- parsnip::fit(iris_reg_wflow, data = training_iris)
iris_reg_wflow_fit
```


## Step 6. Assess

```{r}
# library(workflows)
wf_fit <- iris_reg_wflow_fit %>% 
  pull_workflow_fit()

head(wf_fit$fit$fitted.values)
```



```{r}
predict(iris_reg_wflow_fit, new_data = training_iris)
```

To get more information about the prediction for each sample, we can use the `augment()` function of the broom package. This requires using the preprocessed training data from `bake()` (or with previous versions juice()), as well as the predicted values from either of the two previous methods.

```{r}
wf_fitted_values <- 
  broom::augment(wf_fit$fit, data = preproc_train) %>% 
  select(Sepal.Length, .fitted:.std.resid)

# head(wf_fitted_values)
wf_fitted_values
```


Nice, now we can see what the original value for Sepal.Length right next to the predicted .fitted value, as well as standard errors and other metrics for each value.



Now we can use the `rmse()` function of the `yardstick` package to **compare the truth**, which is the `Sepal.Length` variable, to the _predicted or estimate variable_ which in the previous output is called **`.fitted`**.

We can see that our RMSE was **0.409558**. This is fairly low, so our model did pretty well.

```{r}
yardstick::rmse(wf_fitted_values, 
                truth = Sepal.Length, 
                estimate = .fitted)
```

a plot to see how well we predicted

```{r}
wf_fitted_values %>%
  ggplot(aes(x = Sepal.Length, y = .fitted)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs( x = "True Sepal Length", y = "Predicted Sepal Length")
```

Assuming we are satisfied, we could then perform a final assessment of our model using the testing data.


With the `workflows` package, we can use the splitting information for our original data `split_iris` to fit the final model on the full training set and also on the testing data using the `last_fit()` function of the `tune` package. No preprocessing steps are required.


```{r}
overallfit <-iris_reg_wflow %>%
  tune::last_fit(split_iris)

overallfit
```

```{r}
collect_metrics(overallfit)
```



## Old School

first let's look at what was predicted

```{r}
wf_fitted_values
```


```{r}
oldschool_fit <- lm(iris$Sepal.Length ~ iris$Sepal.Width + iris$Species , data = iris)

summary(oldschool_fit)

tidy(oldschool_fit)
glance(oldschool_fit)
augment(oldschool_fit)
```

